{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMojKB6Ted7VIh+4Y3ihXP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asmaamhidra/projet-AI/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBMlKL0aw7uc",
        "outputId": "8f6c6b0f-de41-4de5-c48e-ac310563e27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 7.0 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp) (4.64.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp) (1.21.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp) (0.3.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Installing collected packages: xxhash, nlp\n",
            "Successfully installed nlp-0.4.0 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyresparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUIcr0Slw9BU",
        "outputId": "eaf64106-297e-4832-b663-a8b94f559f40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyresparser\n",
            "  Downloading pyresparser-1.0.6-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.15.0)\n",
            "Collecting docx2txt>=0.7\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (22.1.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.4.0)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (4.3.3)\n",
            "Requirement already satisfied: cymem>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.0.6)\n",
            "Requirement already satisfied: pyrsistent>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (0.18.1)\n",
            "Requirement already satisfied: nltk>=3.4.3 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.7)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.21.6)\n",
            "Requirement already satisfied: srsly>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.4.4)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (4.64.0)\n",
            "Collecting urllib3>=1.25.3\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 60.5 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six>=20181108\n",
            "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.8.2)\n",
            "Requirement already satisfied: preshed>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.0.7)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.10)\n",
            "Requirement already satisfied: pytz>=2019.1 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2022.2.1)\n",
            "Requirement already satisfied: certifi>=2019.6.16 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2022.6.15)\n",
            "Collecting pycryptodome>=3.8.2\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.23.0)\n",
            "Requirement already satisfied: wasabi>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.3.5)\n",
            "Requirement already satisfied: thinc>=7.0.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (8.1.0)\n",
            "Requirement already satisfied: blis>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (0.7.8)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->pyresparser) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->pyresparser) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->pyresparser) (5.9.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->pyresparser) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.3->pyresparser) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.3->pyresparser) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.3->pyresparser) (7.1.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six>=20181108->pyresparser) (2.1.0)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (2.21)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from preshed>=2.0.1->pyresparser) (1.0.8)\n",
            "Collecting urllib3>=1.25.3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 61.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (0.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (1.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (0.6.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.1.4->pyresparser) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.1.4->pyresparser) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.1.4->pyresparser) (2.0.1)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=e62dc5c217e0ba9f5ddf0d6078fcead6547b789d91ec56f799c179521d6d819d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: urllib3, cryptography, pycryptodome, pdfminer.six, docx2txt, pyresparser\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed cryptography-37.0.4 docx2txt-0.8 pdfminer.six-20220524 pycryptodome-3.15.0 pyresparser-1.0.6 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUAYPIKtxMS5",
        "outputId": "107861ee-ca17-42d9-aee8-11b23c08aaf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20220524)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.1.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (37.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "#Defining the ssential variables to identify phone numbers and email addresses\n",
        "PHONE_REG = re.compile(r'[\\+\\(]?[0-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
        "EMAIL_REG = re.compile(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b')\n",
        "ADRESS_REG = re.compile(r'Address[\\w /,-: ]+[ .]')\n",
        "\n",
        "#Taking user input- file path\n",
        "pdf_path= input(\"Enter your file path:\")\n",
        "\n",
        "#Defining fnction to extraxt the text from pdf\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "#Extracting text from the pdf\n",
        "txt=extract_text_from_pdf(pdf_path)\n",
        "\n",
        "#Opening a new text file to write data\n",
        "text_file=open(\"resume_text.txt\",mode=\"w\",encoding=\"utf-8\")\n",
        "#Write all the data in cv in text filee \n",
        "text_file.write(txt)    \n",
        "#Closing text file\n",
        "text_file.close()\n",
        "\n",
        "#Defining function to extarct phone numbers from text\n",
        "def extract_phone_number(resume_text):\n",
        "    phone = re.findall(PHONE_REG, resume_text)\n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
        "            return number\n",
        "    return None\n",
        "\n",
        "#Defining function to extarct emails from text\n",
        "def extract_emails(resume_text):\n",
        "    return re.findall(EMAIL_REG, resume_text)\n",
        "\n",
        "#Definig function to extract adress from text\n",
        "def extract_adress(resume_text):\n",
        "    return re.findall(ADRESS_REG, resume_text)\n",
        "\n",
        "\n",
        "#Defining function to extarct person names from text\n",
        "person=[]\n",
        "def extract_name(resume_text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(txt)\n",
        "    for token in doc:\n",
        "        if token.pos_=='PROPN':\n",
        "            person.append(token.text)\n",
        "    return person\n",
        "\n",
        "#Calling the functions \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    #Opening a new text file to write data\n",
        "    text_file=open(\"data.txt\",mode=\"w\",encoding=\"utf-8\")\n",
        "    \n",
        "    #Calling extract_name function to extract name\n",
        "    names=extract_name(txt)\n",
        "    if names:\n",
        "        print(\"\\n\")\n",
        "        print('Name: '+ names[0],names[1])\n",
        "        \n",
        "        #Write name on text file\n",
        "        text_file.write(\"Name: \"+ names[0])\n",
        "        text_file.write(\" \"+names[1])\n",
        "    \n",
        "    #Calling extract_emails function to extract emails\n",
        "    emails = extract_emails(txt)\n",
        "    if emails:\n",
        "        print('\\nEmail: '+emails[0])\n",
        "        \n",
        "        #Write emails on text file\n",
        "        text_file.write('\\nEmail: '+emails[0])\n",
        "    \n",
        "    #Calling extract_phone_number function to extract phone number\n",
        "    phone_number = extract_phone_number(txt)\n",
        "    if phone_number:\n",
        "        print('\\nPhone number: '+phone_number)\n",
        "        \n",
        "        #Write phone number on text file\n",
        "        text_file.write('\\nPhone number: '+phone_number )\n",
        "\n",
        "\n",
        "    #Calling extract_adress function to extract adress\n",
        "    adress = extract_adress(txt)\n",
        "    if adress:\n",
        "        print('\\nAdress: '+adress[0])\n",
        "\n",
        "        #Write adress on text file\n",
        "        text_file.write('\\nAdress: '+adress[0])\n",
        "\n",
        "\n",
        "    \n",
        "    #Closing text file\n",
        "    text_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_uWyl6fx3CM",
        "outputId": "0952deda-bd44-456d-98c1-db0b1d727296"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your file path:/content/rajaa cv.pdf\n",
            "\n",
            "\n",
            "Name: Rajaa esstari\n",
            "\n",
            "Email: Rajaes.eg@mail.com\n",
            "\n",
            "Phone number: 9912341209\n",
            "\n",
            "Adress: Address: H No:33-34/2, Chanakya Arcade,Sai Nath Colony Road No:4,Dsnr, Hyderabad \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "M8l6ASP-yBzf"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it3DaIqqyTMU",
        "outputId": "9fdc2000-3ceb-4ff5-97b4-31da6ccaac3c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# you may read the database from a csv file or some other database\n",
        "SKILLS_DB = ['machine learning','data science','Illustrator','python','word','excel','English','C++','Coding']\n",
        "def extract_skills(text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        "    # we create a set to keep the results in.\n",
        "    found_skills = set()\n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in SKILLS_DB:\n",
        "            found_skills.add(token)\n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in SKILLS_DB:\n",
        "            found_skills.add(ngram)\n",
        "\n",
        "    return found_skills\n",
        "\n",
        "text = extract_text_from_pdf('/content/rajaa cv.pdf')\n",
        "skills = extract_skills(txt)\n",
        "print(skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXi42gQhyVaZ",
        "outputId": "2c5782aa-31e4-4269-e438-64a63bbe25e9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Data Science', 'Python', 'Machine Learning'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        " \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "RESERVED_WORDS = ['school','college','university','academy','faculty','institute','Doctorat','Master','licence','BE','B.E.', 'B.E', 'BS', 'B.S', 'ME', 'M.E', 'M.E.', 'MS', 'M.S', 'BTECH', 'B.TECH', 'M.TECH', 'MTECH', 'SSC']\n",
        "def extract_education(text):\n",
        "    organizations = []\n",
        " \n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        "    # we search for each bigram and trigram for reserved words\n",
        "    education = set()\n",
        "    for org in organizations:\n",
        "        for word in RESERVED_WORDS:\n",
        "            if org.lower().find(word) > 0:\n",
        "                education.add(org)\n",
        "\n",
        "    # Extract year\n",
        "    edu={}\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "        if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVXRKkaLzFqQ",
        "outputId": "bb00e9c2-3658-439f-d05c-3325066ead6f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "education_information = extract_education(text)\n",
        "print(education_information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nGtldb-3PRw",
        "outputId": "36604d7a-887b-49b2-d81a-10f555cad033"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Business Analytics Narsee Monjee Institute'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_experience(text):\n",
        "    experience=[]\n",
        "    works=['Ingeneer','Manager','developer','director','president','supervisor','team_leader','teacher']\n",
        "    for work in works:\n",
        "      if work in text:\n",
        "          parse=text(text.index(work))\n",
        "          experience.append(parse)\n",
        "    return experience\n"
      ],
      "metadata": {
        "id": "hL9Yu-4x9eS7"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('               Personal information:           \\n')\n",
        "print('Name: '+ names[0],names[1])\n",
        "print('Email: '+emails[0])\n",
        "print('Phone number: '+phone_number)\n",
        "print('Adress: '+adress[0])\n",
        "print('\\n              Skills:')\n",
        "print(skills)\n",
        "print('\\n              Education:')\n",
        "print(education_information)\n",
        "print('\\n              Experience        \\n')\n",
        "print(experience)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm6E9_sVImAd",
        "outputId": "f298e693-1388-4ef0-f973-df16a9a1fbd0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Personal information:           \n",
            "\n",
            "Name: Rajaa esstari\n",
            "Email: Rajaes.eg@mail.com\n",
            "Phone number: 9912341209\n",
            "Adress: Address: H No:33-34/2, Chanakya Arcade,Sai Nath Colony Road No:4,Dsnr, Hyderabad \n",
            "\n",
            "              Skills:\n",
            "{'Data Science', 'Python', 'Machine Learning'}\n",
            "\n",
            "              Education:\n",
            "{'Business Analytics Narsee Monjee Institute'}\n",
            "\n",
            "              Experience        \n",
            "\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bcJLTid1LkuE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}